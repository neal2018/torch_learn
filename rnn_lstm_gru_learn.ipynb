{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 650\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/anaconda3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/neal/anaconda3/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(lower=True)\n",
    "text8 = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\"./data\",\n",
    "    train=\"text8.train.txt\",\n",
    "    validation=\"text8.dev.txt\",\n",
    "    test=\"text8.test.txt\",\n",
    "    text_field=TEXT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50002\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/anaconda3/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BPTTIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train, val, test = text8\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=device, bptt_len=32, repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five two heaven s in your backyard a film mort liddy wrote the score using a bastardized version of halley s fourth concerto it is mentioned in section one six one john\n",
      "two heaven s in your backyard a film mort liddy wrote the score using a bastardized version of halley s fourth concerto it is mentioned in section one six one john galt\n",
      "galt legends since everyone across the country is asking who is john galt it is not surprising that some people have come up with answers a number of john galt legends are\n",
      "legends since everyone across the country is asking who is john galt it is not surprising that some people have come up with answers a number of john galt legends are told\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/anaconda3/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# show some data\n",
    "it = iter(train_iter)\n",
    "for i in range(2):\n",
    "    batch = next(it)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:, 2].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, nvocab, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(nvocab, ninp)\n",
    "\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type in [\"RNN_TANH\", \"RNN_RELU\"]:\n",
    "            nonlinearity = {\"RNN_TANH\": \"tanh\", \"RNN_RELU\": \"relu\"}[rnn_type]\n",
    "            self.rnn = nn.RNN(\n",
    "                ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"rnn_type should be in ['LSTM', 'GRU', 'RNN_TANH', RNN_RELU']\"\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Linear(nhid, nvocab)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (\n",
    "                weight.new_zeros(\n",
    "                    (self.nlayers, bsz, self.nhid), requires_grad=requires_grad\n",
    "                ),\n",
    "                weight.new_zeros(\n",
    "                    (self.nlayers, bsz, self.nhid), requires_grad=requires_grad\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            return weight.new_zeros(\n",
    "                (self.nlayers, bsz, self.nhid), requires_grad=requires_grad\n",
    "            )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # encoder\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        # rnn\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        # decoder\n",
    "        decoded = self.decoder(output.view(-1, output.shape[2]))\n",
    "\n",
    "        return decoded.view(output.shape[0], output.shape[1], decoded.shape[1]), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_detach(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return [hidden_detach(v) for v in h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, batch_size, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_cnt = 0\n",
    "    it = iter(data)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(batch_size, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            text, target = batch.text.to(device), batch.target.to(device)\n",
    "\n",
    "            output, hidden = model(text, hidden)\n",
    "\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_cnt += np.multiply(*text.size())\n",
    "            total_loss += loss.item() * np.multiply(*text.size())\n",
    "\n",
    "    loss = total_loss / total_cnt\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_TYPE = \"GRU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(RNN_TYPE, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 4.553329944610596\n",
      "best model, val loss:  5.86520219747661\n",
      "epoch 0 iter 100 loss 4.991318702697754\n",
      "epoch 0 iter 200 loss 4.799288272857666\n",
      "epoch 0 iter 300 loss 4.4984965324401855\n",
      "epoch 0 iter 400 loss 4.74827241897583\n",
      "epoch 0 iter 500 loss 5.167263031005859\n",
      "lr decay to 3.125e-05\n",
      "epoch 0 iter 600 loss 5.431086540222168\n",
      "epoch 0 iter 700 loss 5.226358413696289\n",
      "epoch 0 iter 800 loss 4.971348285675049\n",
      "epoch 0 iter 900 loss 5.008023738861084\n",
      "best model, val loss:  5.832428959033384\n",
      "epoch 1 iter 0 loss 4.47780704498291\n",
      "lr decay to 1.5625e-05\n",
      "epoch 1 iter 100 loss 5.025078296661377\n",
      "epoch 1 iter 200 loss 4.740190029144287\n",
      "epoch 1 iter 300 loss 4.463899612426758\n",
      "epoch 1 iter 400 loss 4.672721862792969\n",
      "epoch 1 iter 500 loss 5.118111610412598\n",
      "best model, val loss:  5.826461640470935\n",
      "epoch 1 iter 600 loss 5.341027736663818\n",
      "epoch 1 iter 700 loss 5.158143520355225\n",
      "epoch 1 iter 800 loss 4.933821201324463\n",
      "epoch 1 iter 900 loss 5.026307582855225\n",
      "best model, val loss:  5.82315788875128\n"
     ]
    }
   ],
   "source": [
    "PATH = f\"./saved_model/{RNN_TYPE}_net.pth\"\n",
    "if os.path.exists(PATH):\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "GRAD_CLIP = 1.0\n",
    "EPOCH = 2\n",
    "\n",
    "mini_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "\n",
    "    for i, batch in enumerate(it):\n",
    "        text, target = batch.text.to(device), batch.target.to(device)\n",
    "        hidden = model.init_hidden(BATCH_SIZE)\n",
    "        hidden = hidden_detach(hidden)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text, hidden)\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            val_loss = evaluate(model, val_iter, BATCH_SIZE, loss_fn)\n",
    "\n",
    "            if val_loss < mini_loss:\n",
    "                mini_loss = val_loss\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                print(\"lr decay to\", optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    val_loss = evaluate(model, val_iter, BATCH_SIZE, loss_fn)\n",
    "    if val_loss < mini_loss:\n",
    "        mini_loss = val_loss\n",
    "        print(\"best model, val loss: \", val_loss)\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'RNNModel' object has no attribute 'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a047b12670bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    772\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'RNNModel' object has no attribute 'hidden'"
     ]
    }
   ],
   "source": [
    "model.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = RNNModel(\n",
    "    RNN_TYPE, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5\n",
    ")\n",
    "best_model = best_model.to(device)\n",
    "best_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  338.03785584443256\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_iter, BATCH_SIZE, loss_fn)\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agave <unk> agave laticincta agave harmless agave upscale hurteri mm trel australis lava com which epirus transferred the region upper sea into a small health town to georgia the station it many of the plant lift persians bermuda mt plants actress into central area latitudes or wide basins a chain pronounced <unk> is called lock the ailanthus locomotive while ash is the fruit <unk> antiprotons species north of angola as many uses of permanent paleozoic civilizations and it is not the old name for anagram that see baja with almonds for understanding patting retrieved two five two five zero two\n"
     ]
    }
   ],
   "source": [
    "hidden = best_model.init_hidden(1)\n",
    "gene = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output, hidden = best_model(gene, hidden)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    gene.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22972]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
