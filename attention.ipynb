{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import jieba\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    en = []\n",
    "    cn = []\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            cn.append(\n",
    "                [\"BOS\"] + [c for c in jieba.cut(line[1])] + [\"EOS\"]\n",
    "            )  # jieba.cut generator to list\n",
    "    return en, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    UNK_IDX = 0\n",
    "    PAD_IDX = 1\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s]+=1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    word_dict = {w[0]:index+2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    total_words = len(ls) + 2\n",
    "    return word_dict, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sens, cn_sens, en_dict, cn_dict, sort_by_len=True):\n",
    "    \"\"\"\n",
    "    word to number\n",
    "    \"\"\"\n",
    "    out_en_sens = [[en_dict.get(w, 0) for w in en_sen] for en_sen in en_sens]\n",
    "    out_cn_sens = [[cn_dict.get(w, 0) for w in cn_sen] for cn_sen in cn_sens]\n",
    "    \n",
    "    if sort_by_len:\n",
    "        sorted_index = sorted(range(len(out_en_sens)), key=lambda x: len(out_en_sens[x]))\n",
    "        out_en_sens = [out_en_sens[i] for i in sorted_index]\n",
    "        out_cn_sens = [out_cn_sens[i] for i in sorted_index]\n",
    "    return out_en_sens, out_cn_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(n, sz, shuffle=True):\n",
    "    \"\"\"\n",
    "    seperate range(n) into batches with size of `sz`\n",
    "    \"\"\"\n",
    "    minibatches=[np.arange(idx, min(idx+sz, n)) for idx in range(0, n, sz)]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(minibatches)\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "    \"\"\"\n",
    "    pading seqs to a matrix\n",
    "    \"\"\"\n",
    "    lengths = torch.tensor([len(seq) for seq in seqs])\n",
    "    x = [torch.tensor(seq) for seq in seqs]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    return x_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_examples(en_sens, cn_sens, minibatch_size):\n",
    "    minibatches = get_mini_batches(len(en_sens), minibatch_size)\n",
    "    all_ex=[]\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sents = [en_sens[t] for t in minibatch]\n",
    "        mb_cn_sents = [cn_sens[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sents)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sents)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file = \"data/nmt/en-cn/train.txt\"\n",
    "dev_file = \"data/nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BOS 您 做 什麼 工作 為生 ？ EOS\nBOS what do you do for a living ? EOS\n"
    }
   ],
   "source": [
    "k = 10001\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]]))\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        # mark the end of the sentence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        return out, hid[[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(embedded, y_lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        output = F.log_softmax(self.out(unpacked), -1)\n",
    "        \n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y, y_lengths, hid)\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_len=10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_len):\n",
    "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target, mask):\n",
    "        # x: batch_size * seq_len * vocab_size\n",
    "        x = x.contiguous().view(-1, x.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -x.gather(1, target) * mask\n",
    "        output = torch.sum(output)/torch.sum(mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(f):\n",
    "    def wrapper(model, *args, **kwargs):\n",
    "        PATH = \"./saved_model/no_attention.pth\"\n",
    "        if os.path.exists(PATH):\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "        res = f(model, *args, **kwargs)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load_model\n",
    "def train(model, data, nums_epoches=20):\n",
    "    for epoch in range(nums_epoches):\n",
    "        model.train()\n",
    "        total_num_words = 0\n",
    "        total_loss = 0\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = mb_x.to(device).long()\n",
    "            mb_x_len = mb_x_len.to(device).long()\n",
    "            mb_input = mb_y[:, :-1].to(device).long()\n",
    "            mb_output = mb_y[:, 1:].to(device).long()\n",
    "            mb_y_len = (mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load_model\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = mb_x.to(device).long()\n",
    "            mb_x_len = mb_x_len.to(device).long()\n",
    "            mb_input = mb_y[:, :-1].to(device).long()\n",
    "            mb_output = mb_y[:, 1:].to(device).long()\n",
    "            mb_y_len = (mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device).unsqueeze(0) < mb_y_len.unsqueeze(1)\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0 iteration 0 loss 9.344791412353516\nEpoch 0 iteration 100 loss 5.557158946990967\nEpoch 0 iteration 200 loss 5.195974826812744\nEpoch 0 Training loss 5.895928513083067\nEvaluation loss 5.207137123912003\nEpoch 1 iteration 0 loss 5.177928447723389\nEpoch 1 iteration 100 loss 5.007741928100586\nEpoch 1 iteration 200 loss 4.733947277069092\nEpoch 1 Training loss 4.9570539737987955\nEpoch 2 iteration 0 loss 4.783565044403076\nEpoch 2 iteration 100 loss 4.671952724456787\nEpoch 2 iteration 200 loss 4.449291229248047\nEpoch 2 Training loss 4.634700314986502\nEpoch 3 iteration 0 loss 4.506902694702148\nEpoch 3 iteration 100 loss 4.426587104797363\nEpoch 3 iteration 200 loss 4.198984622955322\nEpoch 3 Training loss 4.397497916280977\nEpoch 4 iteration 0 loss 4.277348041534424\nEpoch 4 iteration 100 loss 4.226634979248047\nEpoch 4 iteration 200 loss 4.011811256408691\nEpoch 4 Training loss 4.193308606073926\nEpoch 5 iteration 0 loss 4.072834014892578\nEpoch 5 iteration 100 loss 4.041784763336182\nEpoch 5 iteration 200 loss 3.8065831661224365\nEpoch 5 Training loss 4.009691120536961\nEvaluation loss 5.207137123912003\nEpoch 6 iteration 0 loss 5.178336143493652\nEpoch 6 iteration 100 loss 5.06544303894043\nEpoch 6 iteration 200 loss 4.78028678894043\nEpoch 6 Training loss 4.991546170133173\nEpoch 7 iteration 0 loss 4.841156005859375\nEpoch 7 iteration 100 loss 4.751345157623291\nEpoch 7 iteration 200 loss 4.501680850982666\nEpoch 7 Training loss 4.693523582527436\nEpoch 8 iteration 0 loss 4.568166732788086\nEpoch 8 iteration 100 loss 4.532442569732666\nEpoch 8 iteration 200 loss 4.304900169372559\nEpoch 8 Training loss 4.4744519863138095\nEpoch 9 iteration 0 loss 4.356459140777588\nEpoch 9 iteration 100 loss 4.32248067855835\nEpoch 9 iteration 200 loss 4.125887870788574\nEpoch 9 Training loss 4.282531417386109\nEpoch 10 iteration 0 loss 4.155086517333984\nEpoch 10 iteration 100 loss 4.1379194259643555\nEpoch 10 iteration 200 loss 3.8970649242401123\nEpoch 10 Training loss 4.105899838552631\nEvaluation loss 5.207137123912003\nEpoch 11 iteration 0 loss 5.1805806159973145\nEpoch 11 iteration 100 loss 5.065068244934082\nEpoch 11 iteration 200 loss 4.783496856689453\nEpoch 11 Training loss 4.991491436764662\nEpoch 12 iteration 0 loss 4.828254699707031\nEpoch 12 iteration 100 loss 4.748449325561523\nEpoch 12 iteration 200 loss 4.514664649963379\nEpoch 12 Training loss 4.69327369745612\nEpoch 13 iteration 0 loss 4.583169460296631\nEpoch 13 iteration 100 loss 4.515720844268799\nEpoch 13 iteration 200 loss 4.291514873504639\nEpoch 13 Training loss 4.471912701798347\nEpoch 14 iteration 0 loss 4.369349002838135\nEpoch 14 iteration 100 loss 4.302272319793701\nEpoch 14 iteration 200 loss 4.0988569259643555\nEpoch 14 Training loss 4.275981172305166\nEpoch 15 iteration 0 loss 4.155223846435547\nEpoch 15 iteration 100 loss 4.1530632972717285\nEpoch 15 iteration 200 loss 3.9184176921844482\nEpoch 15 Training loss 4.095083136627069\nEvaluation loss 5.207137123912003\nEpoch 16 iteration 0 loss 5.171440124511719\nEpoch 16 iteration 100 loss 5.071155071258545\nEpoch 16 iteration 200 loss 4.779055595397949\nEpoch 16 Training loss 4.991828326136268\nEpoch 17 iteration 0 loss 4.832001686096191\nEpoch 17 iteration 100 loss 4.751620292663574\nEpoch 17 iteration 200 loss 4.507392883300781\nEpoch 17 Training loss 4.694619979154349\nEpoch 18 iteration 0 loss 4.5933003425598145\nEpoch 18 iteration 100 loss 4.523481369018555\nEpoch 18 iteration 200 loss 4.30427885055542\nEpoch 18 Training loss 4.476486822150372\nEpoch 19 iteration 0 loss 4.367933750152588\nEpoch 19 iteration 100 loss 4.318417072296143\nEpoch 19 iteration 200 loss 4.116037368774414\nEpoch 19 Training loss 4.284689750430417\nEpoch 20 iteration 0 loss 4.1509222984313965\nEpoch 20 iteration 100 loss 4.149634838104248\nEpoch 20 iteration 200 loss 3.9239184856414795\nEpoch 20 Training loss 4.102326220215666\nEvaluation loss 5.207137123912003\nEpoch 21 iteration 0 loss 5.1769795417785645\nEpoch 21 iteration 100 loss 5.062834739685059\nEpoch 21 iteration 200 loss 4.778761863708496\nEpoch 21 Training loss 4.990667690695538\nEpoch 22 iteration 0 loss 4.828318119049072\nEpoch 22 iteration 100 loss 4.737220287322998\nEpoch 22 iteration 200 loss 4.522628307342529\nEpoch 22 Training loss 4.693312479380215\nEpoch 23 iteration 0 loss 4.577163219451904\nEpoch 23 iteration 100 loss 4.510943412780762\nEpoch 23 iteration 200 loss 4.306629180908203\nEpoch 23 Training loss 4.473685155887912\nEpoch 24 iteration 0 loss 4.370692729949951\nEpoch 24 iteration 100 loss 4.323534965515137\nEpoch 24 iteration 200 loss 4.115242004394531\nEpoch 24 Training loss 4.280891547458347\nEpoch 25 iteration 0 loss 4.130392074584961\nEpoch 25 iteration 100 loss 4.137510299682617\nEpoch 25 iteration 200 loss 3.9303064346313477\nEpoch 25 Training loss 4.097153373905362\nEvaluation loss 5.207137123912003\nEpoch 26 iteration 0 loss 5.165952205657959\nEpoch 26 iteration 100 loss 5.068439960479736\nEpoch 26 iteration 200 loss 4.797821521759033\nEpoch 26 Training loss 4.991343448779406\nEpoch 27 iteration 0 loss 4.835034370422363\nEpoch 27 iteration 100 loss 4.7545857429504395\nEpoch 27 iteration 200 loss 4.507195472717285\nEpoch 27 Training loss 4.695244156647466\nEpoch 28 iteration 0 loss 4.585699081420898\nEpoch 28 iteration 100 loss 4.513440132141113\nEpoch 28 iteration 200 loss 4.3108625411987305\nEpoch 28 Training loss 4.47475371636269\nEpoch 29 iteration 0 loss 4.35863733291626\nEpoch 29 iteration 100 loss 4.327414512634277\nEpoch 29 iteration 200 loss 4.113487720489502\nEpoch 29 Training loss 4.279166444930316\nEpoch 30 iteration 0 loss 4.14539098739624\nEpoch 30 iteration 100 loss 4.12054967880249\nEpoch 30 iteration 200 loss 3.9336512088775635\nEpoch 30 Training loss 4.098040736946223\nEvaluation loss 5.207137123912003\nEpoch 31 iteration 0 loss 5.173868656158447\nEpoch 31 iteration 100 loss 5.05816125869751\nEpoch 31 iteration 200 loss 4.775293827056885\nEpoch 31 Training loss 4.9909822315079735\nEpoch 32 iteration 0 loss 4.83383321762085\nEpoch 32 iteration 100 loss 4.746682643890381\nEpoch 32 iteration 200 loss 4.519057750701904\nEpoch 32 Training loss 4.6946602353404066\nEpoch 33 iteration 0 loss 4.5988240242004395\nEpoch 33 iteration 100 loss 4.528304576873779\nEpoch 33 iteration 200 loss 4.304846286773682\nEpoch 33 Training loss 4.478587421062669\nEpoch 34 iteration 0 loss 4.386452674865723\nEpoch 34 iteration 100 loss 4.341983318328857\nEpoch 34 iteration 200 loss 4.13766622543335\nEpoch 34 Training loss 4.2866586555185835\nEpoch 35 iteration 0 loss 4.163143634796143\nEpoch 35 iteration 100 loss 4.158172130584717\nEpoch 35 iteration 200 loss 3.9367516040802\nEpoch 35 Training loss 4.10692873615984\nEvaluation loss 5.207137123912003\nEpoch 36 iteration 0 loss 5.177265644073486\nEpoch 36 iteration 100 loss 5.072551727294922\nEpoch 36 iteration 200 loss 4.785287380218506\nEpoch 36 Training loss 4.992033805181007\nEpoch 37 iteration 0 loss 4.8326735496521\nEpoch 37 iteration 100 loss 4.745420932769775\nEpoch 37 iteration 200 loss 4.505449295043945\nEpoch 37 Training loss 4.693615549873716\nEpoch 38 iteration 0 loss 4.58219051361084\nEpoch 38 iteration 100 loss 4.518795013427734\nEpoch 38 iteration 200 loss 4.303114414215088\nEpoch 38 Training loss 4.473566613373886\nEpoch 39 iteration 0 loss 4.372802734375\nEpoch 39 iteration 100 loss 4.324449062347412\nEpoch 39 iteration 200 loss 4.118197917938232\nEpoch 39 Training loss 4.281491385821961\nEpoch 40 iteration 0 loss 4.157103061676025\nEpoch 40 iteration 100 loss 4.140488147735596\nEpoch 40 iteration 200 loss 3.9256362915039062\nEpoch 40 Training loss 4.098986800772265\nEvaluation loss 5.207137123912003\nEpoch 41 iteration 0 loss 5.184615612030029\nEpoch 41 iteration 100 loss 5.060458660125732\nEpoch 41 iteration 200 loss 4.782559394836426\nEpoch 41 Training loss 4.991554912187733\nEpoch 42 iteration 0 loss 4.836364269256592\nEpoch 42 iteration 100 loss 4.749362468719482\nEpoch 42 iteration 200 loss 4.5262770652771\nEpoch 42 Training loss 4.693859497070414\nEpoch 43 iteration 0 loss 4.594025611877441\nEpoch 43 iteration 100 loss 4.522984027862549\nEpoch 43 iteration 200 loss 4.305107116699219\nEpoch 43 Training loss 4.47589208324544\nEpoch 44 iteration 0 loss 4.367434501647949\nEpoch 44 iteration 100 loss 4.334139347076416\nEpoch 44 iteration 200 loss 4.097380638122559\nEpoch 44 Training loss 4.28248519350101\nEpoch 45 iteration 0 loss 4.140999794006348\nEpoch 45 iteration 100 loss 4.139556407928467\nEpoch 45 iteration 200 loss 3.9401700496673584\nEpoch 45 Training loss 4.098555701139405\nEvaluation loss 5.207137123912003\nEpoch 46 iteration 0 loss 5.177667617797852\nEpoch 46 iteration 100 loss 5.062513828277588\nEpoch 46 iteration 200 loss 4.771814346313477\nEpoch 46 Training loss 4.991750160230016\nEpoch 47 iteration 0 loss 4.846837043762207\nEpoch 47 iteration 100 loss 4.751820087432861\nEpoch 47 iteration 200 loss 4.51127290725708\nEpoch 47 Training loss 4.694958584452263\nEpoch 48 iteration 0 loss 4.592005252838135\nEpoch 48 iteration 100 loss 4.519607067108154\nEpoch 48 iteration 200 loss 4.309523105621338\nEpoch 48 Training loss 4.474936166087732\nEpoch 49 iteration 0 loss 4.368334770202637\nEpoch 49 iteration 100 loss 4.32988977432251\nEpoch 49 iteration 200 loss 4.127830982208252\nEpoch 49 Training loss 4.279708635599914\nEpoch 50 iteration 0 loss 4.154909610748291\nEpoch 50 iteration 100 loss 4.147840976715088\nEpoch 50 iteration 200 loss 3.9190316200256348\nEpoch 50 Training loss 4.09599384087587\nEvaluation loss 5.207137123912003\nEpoch 51 iteration 0 loss 5.1724629402160645\nEpoch 51 iteration 100 loss 5.055665493011475\nEpoch 51 iteration 200 loss 4.778005599975586\nEpoch 51 Training loss 4.991408811434517\nEpoch 52 iteration 0 loss 4.832518577575684\nEpoch 52 iteration 100 loss 4.744826793670654\nEpoch 52 iteration 200 loss 4.519563674926758\nEpoch 52 Training loss 4.693486426321763\nEpoch 53 iteration 0 loss 4.580251693725586\nEpoch 53 iteration 100 loss 4.5237555503845215\nEpoch 53 iteration 200 loss 4.307992458343506\nEpoch 53 Training loss 4.474133238079024\nEpoch 54 iteration 0 loss 4.350866794586182\nEpoch 54 iteration 100 loss 4.335887908935547\nEpoch 54 iteration 200 loss 4.095090389251709\nEpoch 54 Training loss 4.278849305650477\nEpoch 55 iteration 0 loss 4.157570838928223\nEpoch 55 iteration 100 loss 4.126903533935547\nEpoch 55 iteration 200 loss 3.9302549362182617\nEpoch 55 Training loss 4.098194284093377\nEvaluation loss 5.207137123912003\nEpoch 56 iteration 0 loss 5.17460298538208\nEpoch 56 iteration 100 loss 5.062574863433838\nEpoch 56 iteration 200 loss 4.7778730392456055\nEpoch 56 Training loss 4.99096255818005\nEpoch 57 iteration 0 loss 4.833558559417725\nEpoch 57 iteration 100 loss 4.72567081451416\nEpoch 57 iteration 200 loss 4.502949237823486\nEpoch 57 Training loss 4.693095446392806\nEpoch 58 iteration 0 loss 4.584758281707764\nEpoch 58 iteration 100 loss 4.537837028503418\nEpoch 58 iteration 200 loss 4.315235137939453\nEpoch 58 Training loss 4.471482599895502\nEpoch 59 iteration 0 loss 4.351189136505127\nEpoch 59 iteration 100 loss 4.313880920410156\nEpoch 59 iteration 200 loss 4.1099958419799805\nEpoch 59 Training loss 4.276959744927977\nEpoch 60 iteration 0 loss 4.121170520782471\nEpoch 60 iteration 100 loss 4.111069679260254\nEpoch 60 iteration 200 loss 3.9101076126098633\nEpoch 60 Training loss 4.095052642872278\nEvaluation loss 5.207137123912003\nEpoch 61 iteration 0 loss 5.1757378578186035\nEpoch 61 iteration 100 loss 5.058775424957275\nEpoch 61 iteration 200 loss 4.793953895568848\nEpoch 61 Training loss 4.991619966177212\nEpoch 62 iteration 0 loss 4.833818435668945\nEpoch 62 iteration 100 loss 4.74231481552124\nEpoch 62 iteration 200 loss 4.5091552734375\nEpoch 62 Training loss 4.694998553926865\nEpoch 63 iteration 0 loss 4.594105243682861\nEpoch 63 iteration 100 loss 4.5215744972229\nEpoch 63 iteration 200 loss 4.295053005218506\nEpoch 63 Training loss 4.476723744319683\nEpoch 64 iteration 0 loss 4.372893810272217\nEpoch 64 iteration 100 loss 4.332625865936279\nEpoch 64 iteration 200 loss 4.106212615966797\nEpoch 64 Training loss 4.28222423655277\nEpoch 65 iteration 0 loss 4.148922920227051\nEpoch 65 iteration 100 loss 4.140150547027588\nEpoch 65 iteration 200 loss 3.9283177852630615\nEpoch 65 Training loss 4.10110895230692\nEvaluation loss 5.207137123912003\nEpoch 66 iteration 0 loss 5.174670219421387\nEpoch 66 iteration 100 loss 5.064944267272949\nEpoch 66 iteration 200 loss 4.785062313079834\nEpoch 66 Training loss 4.991214896393903\nEpoch 67 iteration 0 loss 4.840312480926514\nEpoch 67 iteration 100 loss 4.73728084564209\nEpoch 67 iteration 200 loss 4.516800880432129\nEpoch 67 Training loss 4.693976179813014\nEpoch 68 iteration 0 loss 4.5838704109191895\nEpoch 68 iteration 100 loss 4.522761821746826\nEpoch 68 iteration 200 loss 4.303403377532959\nEpoch 68 Training loss 4.473194792683054\nEpoch 69 iteration 0 loss 4.346303939819336\nEpoch 69 iteration 100 loss 4.325985431671143\nEpoch 69 iteration 200 loss 4.112429141998291\nEpoch 69 Training loss 4.277649084735389\nEpoch 70 iteration 0 loss 4.157308578491211\nEpoch 70 iteration 100 loss 4.134939670562744\nEpoch 70 iteration 200 loss 3.936073064804077\nEpoch 70 Training loss 4.0938323227350635\nEvaluation loss 5.207137123912003\nEpoch 71 iteration 0 loss 5.166967391967773\nEpoch 71 iteration 100 loss 5.056981563568115\nEpoch 71 iteration 200 loss 4.7811360359191895\nEpoch 71 Training loss 4.990754327255988\nEpoch 72 iteration 0 loss 4.839388847351074\nEpoch 72 iteration 100 loss 4.741635322570801\nEpoch 72 iteration 200 loss 4.523141384124756\nEpoch 72 Training loss 4.694854559541215\nEpoch 73 iteration 0 loss 4.582818031311035\nEpoch 73 iteration 100 loss 4.53291130065918\nEpoch 73 iteration 200 loss 4.317734718322754\nEpoch 73 Training loss 4.477487000058203\nEpoch 74 iteration 0 loss 4.385372161865234\nEpoch 74 iteration 100 loss 4.337050437927246\nEpoch 74 iteration 200 loss 4.133363723754883\nEpoch 74 Training loss 4.284787946490492\nEpoch 75 iteration 0 loss 4.140030384063721\nEpoch 75 iteration 100 loss 4.135931015014648\nEpoch 75 iteration 200 loss 3.923480749130249\nEpoch 75 Training loss 4.10253289247732\nEvaluation loss 5.207137123912003\nEpoch 76 iteration 0 loss 5.182148456573486\nEpoch 76 iteration 100 loss 5.056578636169434\nEpoch 76 iteration 200 loss 4.774717807769775\nEpoch 76 Training loss 4.991267768833291\nEpoch 77 iteration 0 loss 4.8313374519348145\nEpoch 77 iteration 100 loss 4.745319366455078\nEpoch 77 iteration 200 loss 4.515713691711426\nEpoch 77 Training loss 4.6937947481703315\nEpoch 78 iteration 0 loss 4.570751190185547\nEpoch 78 iteration 100 loss 4.522839546203613\nEpoch 78 iteration 200 loss 4.303170204162598\nEpoch 78 Training loss 4.473095180300681\nEpoch 79 iteration 0 loss 4.386112213134766\nEpoch 79 iteration 100 loss 4.310264587402344\nEpoch 79 iteration 200 loss 4.101889133453369\nEpoch 79 Training loss 4.276410986016808\nEpoch 80 iteration 0 loss 4.138665199279785\nEpoch 80 iteration 100 loss 4.131849765777588\nEpoch 80 iteration 200 loss 3.9140679836273193\nEpoch 80 Training loss 4.091984177852799\nEvaluation loss 5.207137123912003\nEpoch 81 iteration 0 loss 5.182493209838867\nEpoch 81 iteration 100 loss 5.061969757080078\nEpoch 81 iteration 200 loss 4.776259422302246\nEpoch 81 Training loss 4.990837347658188\nEpoch 82 iteration 0 loss 4.836903095245361\nEpoch 82 iteration 100 loss 4.7307658195495605\nEpoch 82 iteration 200 loss 4.517900466918945\nEpoch 82 Training loss 4.692706996664917\nEpoch 83 iteration 0 loss 4.590923309326172\nEpoch 83 iteration 100 loss 4.512979507446289\nEpoch 83 iteration 200 loss 4.335241317749023\nEpoch 83 Training loss 4.474729076084176\nEpoch 84 iteration 0 loss 4.3564581871032715\nEpoch 84 iteration 100 loss 4.322009563446045\nEpoch 84 iteration 200 loss 4.12606954574585\nEpoch 84 Training loss 4.279411600756057\nEpoch 85 iteration 0 loss 4.153336524963379\nEpoch 85 iteration 100 loss 4.147700786590576\nEpoch 85 iteration 200 loss 3.9420952796936035\nEpoch 85 Training loss 4.100609417974888\nEvaluation loss 5.207137123912003\nEpoch 86 iteration 0 loss 5.168692111968994\nEpoch 86 iteration 100 loss 5.068708419799805\nEpoch 86 iteration 200 loss 4.786662578582764\nEpoch 86 Training loss 4.991550769596512\nEpoch 87 iteration 0 loss 4.827327251434326\nEpoch 87 iteration 100 loss 4.741584777832031\nEpoch 87 iteration 200 loss 4.491504669189453\nEpoch 87 Training loss 4.692717546402879\nEpoch 88 iteration 0 loss 4.577983379364014\nEpoch 88 iteration 100 loss 4.520664691925049\nEpoch 88 iteration 200 loss 4.286197185516357\nEpoch 88 Training loss 4.47372576716332\nEpoch 89 iteration 0 loss 4.368198871612549\nEpoch 89 iteration 100 loss 4.349262237548828\nEpoch 89 iteration 200 loss 4.111453533172607\nEpoch 89 Training loss 4.278114559547954\nEpoch 90 iteration 0 loss 4.171600818634033\nEpoch 90 iteration 100 loss 4.1315460205078125\nEpoch 90 iteration 200 loss 3.9331777095794678\nEpoch 90 Training loss 4.096277895294374\nEvaluation loss 5.207137123912003\nEpoch 91 iteration 0 loss 5.178030014038086\nEpoch 91 iteration 100 loss 5.061584949493408\nEpoch 91 iteration 200 loss 4.780429840087891\nEpoch 91 Training loss 4.991325087242006\nEpoch 92 iteration 0 loss 4.83850622177124\nEpoch 92 iteration 100 loss 4.736724853515625\nEpoch 92 iteration 200 loss 4.5205302238464355\nEpoch 92 Training loss 4.6952537423128735\nEpoch 93 iteration 0 loss 4.591032028198242\nEpoch 93 iteration 100 loss 4.521129608154297\nEpoch 93 iteration 200 loss 4.307565689086914\nEpoch 93 Training loss 4.476635838971955\nEpoch 94 iteration 0 loss 4.372879981994629\nEpoch 94 iteration 100 loss 4.351335525512695\nEpoch 94 iteration 200 loss 4.1125030517578125\nEpoch 94 Training loss 4.285798239675111\nEpoch 95 iteration 0 loss 4.17581844329834\nEpoch 95 iteration 100 loss 4.147411346435547\nEpoch 95 iteration 200 loss 3.9413273334503174\nEpoch 95 Training loss 4.105650687822242\nEvaluation loss 5.207137123912003\nEpoch 96 iteration 0 loss 5.175813674926758\nEpoch 96 iteration 100 loss 5.062927722930908\nEpoch 96 iteration 200 loss 4.784724235534668\nEpoch 96 Training loss 4.99107303661946\nEpoch 97 iteration 0 loss 4.8363494873046875\nEpoch 97 iteration 100 loss 4.746154308319092\nEpoch 97 iteration 200 loss 4.500696659088135\nEpoch 97 Training loss 4.69295438382654\nEpoch 98 iteration 0 loss 4.571080207824707\nEpoch 98 iteration 100 loss 4.527325630187988\nEpoch 98 iteration 200 loss 4.293695449829102\nEpoch 98 Training loss 4.471550837223883\nEpoch 99 iteration 0 loss 4.356551170349121\nEpoch 99 iteration 100 loss 4.315667152404785\nEpoch 99 iteration 200 loss 4.11696720123291\nEpoch 99 Training loss 4.274747189674926\n"
    }
   ],
   "source": [
    "train(model, train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BOS you have nice skin . EOS\nBOS 你 的 皮膚 真好 。 EOS\n你的時候你。\n\nBOS you &#39;re UNK correct . EOS\nBOS 你 UNK 正确 。 EOS\n你的狗。\n\nBOS everyone admired his courage . EOS\nBOS 每個 人 都 佩服 他 的 勇氣 。 EOS\n他的人都是我的。\n\nBOS what time is it ? EOS\nBOS 几点 了 ？ EOS\n你的名字？\n\nBOS i &#39;m free tonight . EOS\nBOS 我 今晚 有空 。 EOS\n我不喜欢。\n\nBOS here is your book . EOS\nBOS 這是 你 的 書 。 EOS\n你的人是你的。\n\nBOS they are at lunch . EOS\nBOS 他们 在 吃 午饭 。 EOS\n他們在這裡。\n\nBOS this chair is UNK . EOS\nBOS 這把 椅子 UNK 。 EOS\n这是个的。\n\nBOS it &#39;s pretty heavy . EOS\nBOS 它 UNK 。 EOS\n这是我的。\n\nBOS many attended his funeral . EOS\nBOS 很多 人 都 参加 了 他 的 UNK 。 EOS\n他是个的人。\n\nBOS training will be provided . EOS\nBOS 会 有 训练 。 EOS\n请是个的。\n\nBOS someone is watching you . EOS\nBOS 有人 在 看 著 你 。 EOS\n你的時候你。\n\nBOS i slapped his face . EOS\nBOS 我 摑 了 他 的 臉 。 EOS\n我的朋友。\n\nBOS i like UNK music . EOS\nBOS 我 喜歡 流行 音樂 。 EOS\n我不喜欢。\n\nBOS tom had no children . EOS\nBOS Tom 沒有 孩子 。 EOS\n汤姆在這裡。\n\nBOS please lock the door . EOS\nBOS 請 把 UNK 上 。 EOS\n請把我的。\n\nBOS tom has calmed down . EOS\nBOS 汤姆 冷静下来 了 。 EOS\n汤姆在這裡。\n\nBOS please speak more loudly . EOS\nBOS 請 說 大聲 一點兒 。 EOS\n請你的人都。\n\nBOS keep next sunday free . EOS\nBOS 把 下 周日 空 出来 。 EOS\n他们是个的。\n\nBOS i made a mistake . EOS\nBOS UNK 了 一個 錯 。 EOS\n我在這裡。\n\n"
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}